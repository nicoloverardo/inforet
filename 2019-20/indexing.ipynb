{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Indexing\n",
    "- **Task**: index tokens in documents and weigth their relevance.\n",
    "- **Input**: tokenized documents\n",
    "- **Output**: term-document matrix\n",
    "\n",
    "## Main steps\n",
    "1. Tf (Term frequency)\n",
    "2. Idf (Inverse document frequency)\n",
    "3. TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dataset_file = '../data/wiki_dataset.json'\n",
    "with open(dataset_file, 'r') as infile:\n",
    "    dataset = json.load(infile)\n",
    "docs = dataset['docs']\n",
    "queries = dataset['queries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tokens = lambda text: [x.lemma_ for x in nlp(text) if x.pos_ not in ['PUNCT', 'SPACE'] and not x.is_stop]\n",
    "all_tokens = lambda text: [x.lemma_ for x in nlp(text) if x.pos_ not in ['PUNCT', 'SPACE']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Indexing from scratch\n",
    "Index structure: <code>{doc_id: {token: tf(token, doc_id), ...}, ...}</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tf\n",
    "In this example, we exploit double normalized K tf, with K = 0.5:\n",
    "$$\n",
    "tf(t, d) = K + (1 - K)\\frac{f(t,d)}{\\max\\limits_{t' \\in d} f(t',d)},\n",
    "$$\n",
    "where $f(t,d)$ is the frequency (i.e., count) of token $t$ in document $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "TF, k = {}, 0.5\n",
    "for docid, text in enumerate(docs):\n",
    "    f = Counter(all_tokens(text)).most_common()\n",
    "    maxf = f[0][1]\n",
    "    TF[docid] = dict([(token, k + (1 - k) * (x / maxf)) for token, x in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sabine Bramhoff (born 1 November 1964) is a retired German high jumper. She finished seventh at the 1989 European Indoor Championships. She represented the sports club LC Paderborn, and won the silver medal at the West German champion in 1989. Her personal best jump was 1.94 metres (6.3\\xa0ft), achieved in August 1990 in Düsseldorf.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[124]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1.0),\n",
       " ('-PRON-', 0.875),\n",
       " ('in', 0.875),\n",
       " ('be', 0.75),\n",
       " ('german', 0.75),\n",
       " ('at', 0.75),\n",
       " ('1989', 0.75),\n",
       " ('Sabine', 0.625),\n",
       " ('Bramhoff', 0.625),\n",
       " ('bear', 0.625)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sorted(TF[124].items(), key=lambda x: -x[1]))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idf\n",
    "$$\n",
    "idf(t) = \\log \\left(\\frac{N}{n_t} \\right),\n",
    "$$\n",
    "where $N$ denotes the corpus size, and $n_t$ denotes the number of documents actually containing $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF, N = defaultdict(lambda: 0), len(docs)\n",
    "for k, bow in TF.items():\n",
    "    for t in bow.keys():\n",
    "        DF[t] += 1\n",
    "IDF = lambda x: np.log(N / DF[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2518 8\n",
      "0.21692888706090915 5.968707559985366\n"
     ]
    }
   ],
   "source": [
    "print(DF['the'], DF['chinese'])\n",
    "print(IDF('the'), IDF('chinese'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfIdf = {}\n",
    "for k, bow in TF.items():\n",
    "    TfIdf[k] = dict([(token, w * IDF(token)) for token, w in bow.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bramhoff', 5.030093188540751),\n",
       " ('LC', 5.030093188540751),\n",
       " ('Paderborn', 5.030093188540751),\n",
       " ('1.94', 5.030093188540751),\n",
       " ('ft', 5.030093188540751),\n",
       " ('jumper', 4.596876200690786),\n",
       " ('Indoor', 4.596876200690786),\n",
       " ('6.3', 4.596876200690786),\n",
       " ('Düsseldorf', 4.596876200690786),\n",
       " ('Sabine', 4.16365921284082)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sorted(TfIdf[124].items(), key=lambda x: -x[1]))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiword indexing and compound terms selection\n",
    "**Task**: find sequences of n words (called n_grams) that should be counted as a single word during indexing (i.e., New York).\n",
    "\n",
    "**Approch**: use pointwise mutual information to estimate the probability of a n_gram (say a 2_gram in the example) to be a single compound term\n",
    "\n",
    "$$\n",
    "pmi(t_i, t_j) = \\log \\left (\\frac{P(t_i, t_j)}{P(t_i)P(t_j)} \\right ) \n",
    "$$\n",
    "\n",
    "Denoting $f(t)$ the frequency of terms in the corpus, probabilities can be estimated as:\n",
    "\n",
    "$$\n",
    "P(t_i, t_j) = \\frac{f(t_i, t_j)}{\\sum\\limits_{(x, y) \\in corpus}f(x, y)} \n",
    ", P(t_i) = \\frac{f(t_i)}{\\sum\\limits_{t \\in corpus}f(t)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unigram prebability estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Un = defaultdict(lambda: 0), 0\n",
    "for doc in docs:\n",
    "    for token in tokens(doc):\n",
    "        U[token] += 1\n",
    "        Un += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_u = lambda x: U[x] / Un"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 129076 0.0007592426167529208\n"
     ]
    }
   ],
   "source": [
    "print(U['school'], Un, p_u('school'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigram probability estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('this', 'is', 'a'), ('is', 'a', 'sentence'), ('a', 'sentence', 'for'), ('sentence', 'for', 'example')]\n"
     ]
    }
   ],
   "source": [
    "print([(a, b, c) for a, b, c in nltk.ngrams('this is a sentence for example'.split(), 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, Bn = defaultdict(lambda: 0), 0\n",
    "for doc in docs:\n",
    "    for a, b in nltk.ngrams(tokens(doc), 2):\n",
    "        B[(a, b)] += 1\n",
    "        Bn += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_b = lambda x, y: B[(x, y)] / Bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204 125948 0.0016197160733000921\n"
     ]
    }
   ],
   "source": [
    "print(B[('New', 'York')], Bn, p_b('New', 'York'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "PMI = {}\n",
    "for (a, b), _ in B.items():\n",
    "    PMI[(a, b)] = np.log(p_b(a, b) / (p_u(a) * p_u(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glaucium stigma 11.792688911965513 1 1\n",
      "stigma lobe 11.792688911965513 1 1\n",
      "lactucoide Himalayan 11.792688911965513 1 1\n",
      "subgroup Rhaetian 11.792688911965513 1 1\n",
      "belltower campanile 11.792688911965513 1 1\n",
      "triassic sedimentary 11.792688911965513 1 1\n",
      "illustrious forebear 11.792688911965513 1 1\n",
      "Mayora Rengifo 11.792688911965513 1 1\n",
      "UT Cajamarca 11.792688911965513 1 1\n",
      "physalaemus maculiventris 11.792688911965513 1 1\n"
     ]
    }
   ],
   "source": [
    "for (a, b), p in sorted(PMI.items(), key=lambda x: -x[1])[:10]:\n",
    "    print(a, b, p, U[a], U[b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a threshold on the miminum number of occurrences requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PMI, th = {}, 5\n",
    "for (a, b), _ in B.items():\n",
    "    if U[a] > th and U[b] > th:\n",
    "        PMI[(a, b)] = np.log(p_b(a, b) / (p_u(a) * p_u(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hong Kong 10.000929442737458 6 6\n",
      "Franche Comté 10.000929442737458 6 6\n",
      "Fissurellidae keyhole 9.8467787629102 6 7\n",
      "Serbian Cyrillic 9.818607885943504 6 6\n",
      "Northwest Territories 9.713247370285677 8 7\n",
      "Singles Chart 9.664457206116245 7 6\n",
      "Pearl Jam 9.664457206116245 7 6\n",
      "Pyramidellidae pyram 9.595464334629295 8 9\n",
      "Terebridae auger 9.595464334629295 9 9\n",
      "Buenos Aires 9.490103818971468 10 10\n",
      "Plot E 9.441313654802036 6 7\n",
      "Middleton Prestwich 9.307782262177513 10 6\n",
      "Bourgogne Franche 9.307782262177513 12 6\n",
      "sudanese Canadians 9.307782262177513 6 6\n",
      "Brandy Melville 9.307782262177513 8 6\n",
      "Copa América 9.287162974974777 7 7\n",
      "terrestrial pulmonate 9.25899209800808 9 7\n",
      "Trinidad Tobago 9.227739554503977 13 7\n",
      "Costa Rica 9.227739554503977 13 9\n",
      "keyhole limpet 9.227739554503977 7 13\n"
     ]
    }
   ],
   "source": [
    "for (a, b), p in sorted(PMI.items(), key=lambda x: -x[1])[:20]:\n",
    "    print(a, b, p, U[a], U[b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed bigrams in tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi_tokenizer(doc, perc=95):\n",
    "    bigram = []\n",
    "    words = []\n",
    "    tks = tokens(doc)\n",
    "    th = np.percentile(list(PMI.values()), perc)\n",
    "    for (a, b) in nltk.ngrams(tks, 2):\n",
    "        if (a, b) in PMI.keys() and PMI[(a, b)] > th:\n",
    "            if len(bigram) == 0:\n",
    "                bigram += [a, b]\n",
    "            else:\n",
    "                bigram.append(b)\n",
    "        else:\n",
    "            if len(bigram) > 0:\n",
    "                words.append(\" \".join(bigram))\n",
    "                bigram = []\n",
    "            else:\n",
    "                words.append(a)\n",
    "    if tks[-1] != words[-1].split()[-1]:\n",
    "        words.append(tks[-1])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_tokens = lambda doc: pmi_tokenizer(doc, perc=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model', 'China', 'Special', 'Administrative', 'Regions', 'SARs', 'Hong', 'Kong', 'Macau', 'like']\n",
      "['Regions', 'SARs', 'Hong Kong', 'Macau', 'like', 'Basic', 'Law', '기본법', 'Kibonpŏp', 'chinese']\n"
     ]
    }
   ],
   "source": [
    "print(tokens(docs[180])[40:50])\n",
    "print(pmi_tokens(docs[180])[40:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = pd.DataFrame(TfIdf)\n",
    "M.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25068, 3128)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3118</th>\n",
       "      <th>3119</th>\n",
       "      <th>3120</th>\n",
       "      <th>3121</th>\n",
       "      <th>3122</th>\n",
       "      <th>3123</th>\n",
       "      <th>3124</th>\n",
       "      <th>3125</th>\n",
       "      <th>3126</th>\n",
       "      <th>3127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>be</td>\n",
       "      <td>0.001920</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.00144</td>\n",
       "      <td>0.001920</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.00192</td>\n",
       "      <td>0.001920</td>\n",
       "      <td>0.001920</td>\n",
       "      <td>0.001920</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>0.00192</td>\n",
       "      <td>0.001920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>South</td>\n",
       "      <td>2.789724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.510752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Carolina</td>\n",
       "      <td>4.093879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>the</td>\n",
       "      <td>0.180774</td>\n",
       "      <td>0.216929</td>\n",
       "      <td>0.216929</td>\n",
       "      <td>0.216929</td>\n",
       "      <td>0.216929</td>\n",
       "      <td>0.216929</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.180774</td>\n",
       "      <td>0.216929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198851</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.216929</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.135581</td>\n",
       "      <td>0.162697</td>\n",
       "      <td>0.162697</td>\n",
       "      <td>0.162697</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.144619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Great</td>\n",
       "      <td>3.517040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5        6     \\\n",
       "be        0.001920  0.001600  0.001440  0.001680  0.001680  0.001280  0.00144   \n",
       "South     2.789724  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
       "Carolina  4.093879  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
       "the       0.180774  0.216929  0.216929  0.216929  0.216929  0.216929  0.00000   \n",
       "Great     3.517040  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
       "\n",
       "              7         8         9     ...      3118    3119      3120  \\\n",
       "be        0.001920  0.001600  0.001536  ...  0.001600  0.0016  0.001303   \n",
       "South     0.000000  0.000000  0.000000  ...  0.000000  0.0000  0.000000   \n",
       "Carolina  0.000000  0.000000  0.000000  ...  0.000000  0.0000  0.000000   \n",
       "the       0.135581  0.180774  0.216929  ...  0.198851  0.0000  0.216929   \n",
       "Great     0.000000  0.000000  0.000000  ...  0.000000  0.0000  0.000000   \n",
       "\n",
       "             3121      3122      3123      3124      3125     3126      3127  \n",
       "be        0.00192  0.001920  0.001920  0.001920  0.001440  0.00192  0.001920  \n",
       "South     0.00000  0.000000  2.510752  0.000000  0.000000  0.00000  0.000000  \n",
       "Carolina  0.00000  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  \n",
       "the       0.00000  0.135581  0.162697  0.162697  0.162697  0.00000  0.144619  \n",
       "Great     0.00000  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  \n",
       "\n",
       "[5 rows x 3128 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>be</th>\n",
       "      <th>South</th>\n",
       "      <th>Carolina</th>\n",
       "      <th>the</th>\n",
       "      <th>Great</th>\n",
       "      <th>Falls</th>\n",
       "      <th>a</th>\n",
       "      <th>town</th>\n",
       "      <th>in</th>\n",
       "      <th>Chester</th>\n",
       "      <th>...</th>\n",
       "      <th>Whittaker</th>\n",
       "      <th>Masoala</th>\n",
       "      <th>kona</th>\n",
       "      <th>palm</th>\n",
       "      <th>Ochil</th>\n",
       "      <th>Jamestown</th>\n",
       "      <th>Steuben</th>\n",
       "      <th>Morehouse</th>\n",
       "      <th>Artanovsky</th>\n",
       "      <th>225</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00192</td>\n",
       "      <td>2.789724</td>\n",
       "      <td>4.093879</td>\n",
       "      <td>0.180774</td>\n",
       "      <td>3.51704</td>\n",
       "      <td>3.979138</td>\n",
       "      <td>0.109551</td>\n",
       "      <td>2.282118</td>\n",
       "      <td>0.159613</td>\n",
       "      <td>3.655466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.00160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216929</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.00144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216929</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.00168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216929</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.143785</td>\n",
       "      <td>2.139485</td>\n",
       "      <td>0.179565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.00168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216929</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25068 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        be     South  Carolina       the    Great     Falls         a  \\\n",
       "0  0.00192  2.789724  4.093879  0.180774  3.51704  3.979138  0.109551   \n",
       "1  0.00160  0.000000  0.000000  0.216929  0.00000  0.000000  0.000000   \n",
       "2  0.00144  0.000000  0.000000  0.216929  0.00000  0.000000  0.102704   \n",
       "3  0.00168  0.000000  0.000000  0.216929  0.00000  0.000000  0.143785   \n",
       "4  0.00168  0.000000  0.000000  0.216929  0.00000  0.000000  0.123244   \n",
       "\n",
       "       town        in   Chester  ...  Whittaker  Masoala  kona  palm  Ochil  \\\n",
       "0  2.282118  0.159613  3.655466  ...        0.0      0.0   0.0   0.0    0.0   \n",
       "1  0.000000  0.000000  0.000000  ...        0.0      0.0   0.0   0.0    0.0   \n",
       "2  0.000000  0.134674  0.000000  ...        0.0      0.0   0.0   0.0    0.0   \n",
       "3  2.139485  0.179565  0.000000  ...        0.0      0.0   0.0   0.0    0.0   \n",
       "4  0.000000  0.000000  0.000000  ...        0.0      0.0   0.0   0.0    0.0   \n",
       "\n",
       "   Jamestown  Steuben  Morehouse  Artanovsky  225  \n",
       "0        0.0      0.0        0.0         0.0  0.0  \n",
       "1        0.0      0.0        0.0         0.0  0.0  \n",
       "2        0.0      0.0        0.0         0.0  0.0  \n",
       "3        0.0      0.0        0.0         0.0  0.0  \n",
       "4        0.0      0.0        0.0         0.0  0.0  \n",
       "\n",
       "[5 rows x 25068 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.T.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing with scikit-learn\n",
    "A tutorial on scikit-learn text processing is available [here](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn text facilities expect to work with text as strings instead of pre-tokenized text. Thus, we create a pseudo-text by exploiting our previous tokenizers and creating pseudo words for bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_docs = [\" \".join([x.replace(' ', '_') for x in pmi_tokens(d)]) for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sinŭiju Special Administrative Region special administrative region SAR North Korea proclaim 2002 de facto operation 2014 border China establish September 2002 area include part Sinŭiju surround area attempt introduce market economic_directly_govern_case directly_govern Cities special administrative region model China Special Administrative Regions SARs Hong_Kong Macau like Basic Law 기본법 Kibonpŏp chinese dutch businessman Yang Bin appoint governor SPA Presidium 2002 formally assume post arrest_chinese_authority_sentence 18 year prison_tax evasion economic_crime north korean_authority soon announce development sinŭiju SAR continue SAR administration_Commission Foreign Economic Cooperation Promotion plan SAR abandon April 2008 SAR reform_effect widely believe North Korea_abandon_project governor_arrest Julie Sa 沙日香 appoint governor 2004'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo_docs[180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = V.fit_transform(pseudo_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3128x25779 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 96325 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map matrix columns on words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11514"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.vocabulary_.get('hong_kong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 180, 1418, 1792, 2749]), array([0, 0, 0, 0]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(C[:,11514].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[1418, 11514]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfTransformer(use_idf=True)\n",
    "X = tf_idf.fit_transform(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3128x25779 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 96325 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08535316687415362"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[180, 11514]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.09586641,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[180,:].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "221.25px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
